
"""Coronet servidor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OVyUmPP6EQXxrYPNqz9vfmoeHogpZg5A

CoroNet: COVID19 Chest XRays classification model 
By Dr Asif Iqbal Khan


The model uses Xception/InceptionResNetV2/NasNet as base model pre-trained on ImageNet, retrained on Chest X-rays dataset
"""


#pip install pandas
#pip install pillow
#pip install -U scikit-learn


#import libraries and packages

import h5py
from numpy.random import seed
seed(8) #1

import tensorflow
tensorflow.random.set_seed(7)

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os

from tensorflow.keras import backend as K
from tensorflow.keras.models import Model ,load_model
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.applications.vgg16 import decode_predictions
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint
import numpy as np
import tensorflow as tf

from tensorflow.python.keras import models
from tensorflow.python.keras import layers

from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import CSVLogger 

"""List of parameters data augmentation"""
vertical_flip_list= [True,True,True,False,False,True,True,True,False,True,True,True,True,False,False,True,False,False,True,True,False,True,True,False,False,False,False,True,True,False,False,True,False,True,False,False,True,False,False,False,True,False,True,False,True,True,True,False,True,False,False,True,True,False,False,False,True,False,False,True,False,True,False,True]
horizontal_flip_list= [True,True,True,True,False,True,False,True,False,True,False,True,False,True,False,True,True,True,True,False,False,True,False,False,False,True,False,False,True,False,True,True,False,True,False,True,False,False,True,True,False,True,False,False,False,False,True,True,True,False,False,False,True,True,True,False,False,False,True,False,True,False,True,False]
rotation_range_list= [15,0,15,15,15,0,15,15,15,0,15,0,15,0,0,0,0,15,0,0,0,15,0,0,15,0,15,0,15,15,0,15,15,0,0,15,15,15,0,15,15,0,15,0,0,0,0,15,15,0,0,15,15,0,0,15,0,0,15,0,15,0,15,15]
zoom_range_list= [0,0.15,0.15,0,0,0,0,0.15,0,0,0,0.15,0,0.15,0.15,0.15,0.15,0.15,0,0,0,0,0.15,0,0.15,0.15,0.15,0.15,0,0.15,0,0.15,0,0,0.15,0,0.15,0,0,0,0.15,0.15,0.15,0,0,0.15,0.15,0.15,0,0.15,0.15,0,0.15,0,0,0.15,0,0,0,0,0.15,0.15,0.15,0.15]
width_shift_range_list= [0.2,0,0.2,0,0.2,0.2,0,0,0,0,0.2,0.2,0.2,0,0.2,0,0.2,0.2,0.2,0.2,0,0.2,0.2,0.2,0,0.2,0.2,0.2,0,0.2,0,0,0,0,0.2,0.2,0.2,0.2,0.2,0.2,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0.2,0,0.2,0,0,0,0,0.2,0,0,0.2,0.2]
shear_range_list= [0,0,0.25,0,0,0.25,0.25,0.25,0.25,0,0.25,0,0,0,0.25,0.25,0,0.25,0,0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0.25,0,0.25,0,0.25,0,0,0,0.25,0.25,0,0.25,0,0.25,0.25,0.25,0.25,0,0,0,0.25,0.25,0,0,0,0.25,0.25,0,0.25,0,0.25]                 
height_shift_range_list= [0.2,0,0.2,0.2,0.2,0.2,0,0,0.2,0.2,0.2,0.2,0,0.2,0,0.2,0,0,0,0.2,0,0,0.2,0.2,0,0.2,0.2,0,0,0,0,0.2,0,0,0.2,0.2,0.2,0,0.2,0,0,0,0.2,0,0.2,0.2,0,0.2,0.2,0.2,0,0.2,0,0.2,0,0.2,0,0.2,0,0,0,0,0.2,0]
  


for x in range(len(vertical_flip_list)):
  print(os.listdir("/home/imhidalgo/Desktop"))
  print("Total de corridas faltantes " + str(len(vertical_flip_list)-x))
  """**Crete Data Generators to preprocess and prepare training and validation**"""

  from os import listdir
  data_list = listdir('/home/imhidalgo/Desktop/covid-19/four_classes/train')
  print(len(data_list))


  DATASET_PATH  = '/home/imhidalgo/Desktop/covid-19/four_classes/train'
  test_dir =  '/home/imhidalgo/Desktop/covid-19/four_classes/test'
  IMAGE_SIZE    = (150, 150)
  NUM_CLASSES   = len(data_list)
  BATCH_SIZE    = 25  # try reducing batch size or freeze more layers if your GPU runs out of memory
  NUM_EPOCHS    = 300 #300
  LEARNING_RATE =0.0001
  """if x == 0:
    #Train datagen here is a preprocessor
    print(x)
    print("corrida")
    train_datagen = ImageDataGenerator(rescale=1./255,
                                       #rotation_range=50,
                                       featurewise_center = True,
                                       featurewise_std_normalization = True,
                                       #width_shift_range=0.2,
                                       #height_shift_range=0.2,
                                       #shear_range=0.25,
                                       #zoom_range=0.1,
                                       zca_whitening = True,
                                       #channel_shift_range = 20,
                                       #horizontal_flip = True ,
                                       #vertical_flip = True ,
                                       validation_split = 0.2,
                                       fill_mode='constant')"""

  #Train datagen here is a preprocessor
  print("%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Run "+str(x+1))
  train_datagen = ImageDataGenerator(rescale=1./255,
                                     rotation_range=rotation_range_list[x],
                                     featurewise_center = True,
                                     featurewise_std_normalization = True,
                                     width_shift_range=width_shift_range_list[x],
                                     height_shift_range=height_shift_range_list[x],
                                     shear_range=shear_range_list[x],
                                     zoom_range=zoom_range_list[x],
                                     zca_whitening = True,
                                     #channel_shift_range = 20,
                                     horizontal_flip=horizontal_flip_list[x],
                                     vertical_flip=vertical_flip_list[x],
                                     validation_split = 0.2,
                                     fill_mode='constant')

  print("rotation_range = "+ str(rotation_range_list[x]))
  print("width_shift_range = "+ str(width_shift_range_list[x]))
  print("height_shift_range = "+ str(height_shift_range_list[x]))
  print("shear_range = "+ str(shear_range_list[x]))
  print("zoom_range = "+ str(zoom_range_list[x]))
  print("horizontal_flip = "+ str(horizontal_flip_list[x]))
  print("vertical_flip = "+ str(vertical_flip_list[x]))


  # For multiclass use categorical n for binary use binary
  train_batches = train_datagen.flow_from_directory(DATASET_PATH,
                                                    target_size=IMAGE_SIZE,
                                                    shuffle=True,
                                                    batch_size=BATCH_SIZE,
                                                    subset = "training",
                                                    seed=42,
                                                    class_mode="categorical"   #For multiclass use categorical n for binary use binary
                                                    )

  valid_batches = train_datagen.flow_from_directory(DATASET_PATH,
                                                    target_size=IMAGE_SIZE,
                                                    shuffle=True,
                                                    batch_size=BATCH_SIZE,
                                                    subset = "validation",
                                                    seed=42,
                                                    class_mode="categorical"  #For multiclass use categorical n for binary use binary
                                                   
                                                    )

  

  """Create a new model or Use Pre trained CNN Model and modify according to the need. """

  #Simple CNN model based on Xception. Set dense layer neuron count same as the no. of output classes 
  #If you wnna use a saved model then skip this step


  from tensorflow.keras.applications import Xception

  conv_base = Xception(weights='imagenet',
                    include_top=False,
                    input_shape=(150, 150, 3))


  conv_base.trainable = True


  model = models.Sequential()
  model.add(conv_base)




  model.add(layers.Flatten())
  model.add(layers.Dropout(0.5))
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dense(3, activation='softmax')) #3

  model.compile(optimizer= 'adam' , loss= 'categorical_crossentropy', metrics=['accuracy',
                                                                               tf.keras.metrics.CategoricalAccuracy(),
                                                                               tf.keras.metrics.Precision(),
                                                                               tf.keras.metrics.Recall()])

  """**Load an already saved model (If there is a trained model already saved on drive)**"""

  #Load saved model from .hs file, otherwise disable this line
  #result=load_model('/home/imhidalgo/Desktop/3-class-Covid19-Mod-Xception.h5')
  #print(result.summary())

  logger = "/home/imhidalgo/Desktop/" + str(x+1) + "-training.log" 
  
  #if x==0:
   # weights_name='/home/imhidalgo/Desktop/C1'
          
  #if x==1:
   # weights_name='/home/imhidalgo/Desktop/C2'
          
  
  csv_logger = CSVLogger(logger)
  

  #FIT MODEL
  print(len(train_batches))
  print(len(valid_batches))

  STEP_SIZE_TRAIN=train_batches.n//train_batches.batch_size
  STEP_SIZE_VALID=valid_batches.n//valid_batches.batch_size

  result=model.fit(train_batches,
                          steps_per_epoch =STEP_SIZE_TRAIN,
                          validation_data = valid_batches,
                          validation_steps = STEP_SIZE_VALID,
                          epochs= NUM_EPOCHS,
                          callbacks=[csv_logger]
                         )

  
  
  model_h5 = 'C' + str(x+1) + '-3-class-Covid19-Mod-Xception.h5'
  model.save(model_h5)

  """Evaluation"""
  
  

  #Plot the accuracy and loss graphs
  test_datagen = ImageDataGenerator(rescale=1. / 255)
  test_dir =  '/home/imhidalgo/Desktop/covid-19/four_classes/test'
  eval_generator = test_datagen.flow_from_directory(test_dir,target_size=IMAGE_SIZE,batch_size=1, 
                                                    shuffle=False, seed=42, class_mode="categorical")
  eval_generator.reset()    
  eval_generator.reset()  
  x = model.evaluate_generator(eval_generator,
                             steps = np.ceil(len(eval_generator)), 
                             use_multiprocessing = False,
                             verbose = 1,
                             workers=1,
                             )


  print('Test loss:' , x[0])
  print('Test accuracy:',x[1])

  IMAGE_SIZE    = (150, 150)
  test_datagen = ImageDataGenerator(rescale=1. / 255)
  test_dir =  '/home/imhidalgo/Desktop/covid-19/four_classes/test'
  pred_generator = test_datagen.flow_from_directory(
          test_dir,target_size=IMAGE_SIZE,
          batch_size=1,
          shuffle=False,
          
          seed=42,
          
          class_mode="categorical")
  pred_generator.reset()   

  count=[0,0,0,0]

  files=pred_generator.filenames

  for i in range(len(files)):
    x,y = pred_generator.next()
    img = x
    predict=model.predict(img)
    
    p=np.argmax(predict, axis=-1)
    print(str(p[0])+" "+files[pred_generator.batch_index-1])
    print(predict)
    p=model.predict_classes(img)
    count[p[0]]+=1
    
  print(str(p[0])+" "+files[i]) 
  print(count)

  from sklearn.metrics import confusion_matrix
  from sklearn.metrics import plot_confusion_matrix
  from sklearn.metrics import classification_report
  filenames = eval_generator.filenames
  nb_samples = len(filenames)
  eval_generator.reset()
  predict = model.predict_generator(eval_generator,steps = np.ceil(len(eval_generator)))
  pp=predict
  predict=np.argmax(predict, axis=-1)
  classes= eval_generator.classes[eval_generator.index_array]
  acc=sum(predict==classes)/len(predict)
  names=["covid","normal","pneumonia"]
  print(confusion_matrix(classes,predict))
  cm = confusion_matrix(classes, predict)
  print(cm)
  print(classification_report(classes,predict))

